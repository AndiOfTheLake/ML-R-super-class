---
title: 'Supervised Learning in R: Classification'
author: "Andi"
Last updated: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# {r, echo = FALSE, results='hide'}
# if we used both 'echo=TRUE' and 'results=hide' the pipe would not work properly
# if we used 'echo = FALSE' and 'results=hide' we would have only messages (i.e. attaching package) If we don't want them we set 'error = FALSE', 'warning = FALSE', and 'message = FALSE'.
library(dplyr)
```

Supervised learning: train a machine to learn from prior examples. When the concept to be learned.

## Classification with Nearest Neighbors

## Recognizing a road sign with kNN

```{r}
# Load the 'class' package
library(class)

# load datasets
signs<-read.csv("signs.csv")
next_sign<-read.csv("next_sign.csv")

str(signs)
str(next_sign)

# Create a vector of labels
sign_types <- signs$sign_type

# Classify the next sign observed
# Note that we need to remove the first column from the "signs" dataset 
# because this columns is "sign_types".
# If we do not get rid of the column we run into coercion problems
knn(train = signs[-1], test = next_sign, cl = sign_types)
```

## Exploring the traffic sign dataset

```{r}
# Examine the structure of the signs dataset
str(signs)

# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

As expected, stop signs tend to have a higher average red value. This is how kNN identifies similar signs.

## Classifying a collection of road signs

```{r}
test_signs<-read.csv("test_signs.csv")

# Use kNN to identify the test road signs
sign_types <- signs$sign_type
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)

# Create a confusion matrix of the predicted versus actual values
signs_actual <- test_signs$sign_type
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

## The k of knn

With smaller neighborhoods, kNN can identify more subtle patterns in the data.

## Testing other 'k' values

```{r}
signs_test<-test_signs # They renamed the dataframe on Data Camp

# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types)
mean(k_1 == signs_actual)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7)
mean(k_7 == signs_actual)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 15)
mean(k_15 == signs_actual)
```

## Seeing how the neighbors voted

To see which predictions are close(r) to unanimous.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, prob = TRUE, k = 7)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```

## Data preparation for kNN

normalization

## Understanding Bayesian methods

```{r}
library(naivebayes)
```

## Computing probabilities

```{r}
where9am<-read.csv("where9am.csv")
str(where9am)
str(where9am$daytype)
str(where9am$location)
table(where9am$daytype)
table(where9am$location)

# Compute P(A) 
p_A <- nrow(subset(where9am, location == "office"))/nrow(where9am)

# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday"))/nrow(where9am)

# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office", daytype == "weekday"))/nrow(where9am)

# Compute P(A | B) and print its value
p_A_given_B <- p_AB/p_B
p_A_given_B
```

## A simple Naive Bayes location model

```{r}
# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Predict Thursday's 9am location
# First create "newdata"
thursday9am<-factor(c("weekday"), levels = c("weekday", "weekend")) %>% data.frame()
names(thursday9am)<-"daytype"; thursday9am

predict(locmodel, newdata = thursday9am)

# Predict Saturdays's 9am location
saturday9am<-factor(c("weekend"), levels = c("weekday", "weekend")) %>% data.frame()
names(saturday9am)<-"daytype"; saturday9am

predict(locmodel, newdata = saturday9am)
```

## Examining "raw" probabilities

```{r}
# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
locmodel

# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am , type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am , type = "prob")
```

## Understanding NB's "naivety"

```{r}
locations<-read.csv("locations.csv")
head(locations)

# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, 
data = locations)

# Predict Brett's location on a weekday afternoon
weekday_afternoon <- locations[13,]
predict(locmodel, newdata = weekday_afternoon)

# Predict Brett's location on a weekday evening
weekday_evening <- locations[19,]
predict(locmodel, newdata = weekday_evening)
```

## Preparing for unforeseen circumstances

```{r}
# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
weekend_afternoon<-locations[85,]
predict(locmodel, newdata = weekend_afternoon, type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data = locations, laplace = 1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, newdata = weekend_afternoon, type = "prob")
```

## Making binary predictions with regression

## Building simple logistic regression models

```{r}
donors<-read.csv("donors.csv")
dim(donors)
head(donors)

# Examine the dataset to identify potential independent variables
str(donors)

# Explore the dependent variable
table(donors$donated)

# Build the donation model
donation_model <- glm(donated ~ bad_address 
+ interest_religion + interest_veterans, 
                      data = donors, family = "binomial")

# Summarize the model results
summary(donation_model)
```

## Making a binary prediction

```{r}
# Estimate the donation probability
donors$donation_prob <- predict(donation_model, type = "response")

# Find the donation probability of the average prospect
mean(donors$donated)

# Predict a donation if probability of donation is greater than average (0.0504)
donors$donation_pred <- ifelse(donors$donation_prob > 0.0504, 1, 0)

# Calculate the model's accuracy
mean(donors$donation_pred == donors$donated)

# Note: notice that mean(donors$donated) = mean(donors$donation_prob). Think why.
mean(donors$donation_prob)
```

## The limitations of accuracy

In the previous exercise, you found that the logistic regression model made a correct prediction nearly 80% of the time. Despite this relatively high accuracy, the result is misleading due to the rarity of outcome being predicted.

The `donors` dataset is available in your workspace. What would the accuracy have been if a model had simply predicted "no donation" for each person?

```{r}
1 - sum(donors$donated)/length(donors$donated)
```

With an accuracy of only 80%, the model is actually performing WORSE than if it were to predict non-donor for every record. This example shows that accuracy is a very misleading measure of model performance on imbalanced datasets

## Model performance tradeoffs

## Calculating ROC Curves and AUC

```{r}
# Load the pROC package
library(pROC)

# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_prob)

# Plot the ROC curve
plot(ROC, col = "blue")

# Calculate the area under the curve (AUC)
auc(ROC)
```

## Dummy variables, missing data, and interactions

## Coding categorical features

```{r}
# Convert the wealth rating to a factor
donors$wealth_levels <- factor(donors$wealth_rating, levels = c(0, 1, 2, 3), labels = c("Unknown", "Low", "Medium", "High"))

# Use relevel() to change reference category
donors$wealth_levels <- relevel(donors$wealth_levels, ref = "Medium")

# See how our factor coding impacts the model
summary(glm(donated ~ wealth_levels, data = donors, family = "binomial"))
```

## Handling missing data

```{r}
# Find the average age among non-missing values
summary(donors$age)

# Impute missing age values with the mean age
donors$imputed_age <- ifelse(is.na(donors$age), round(mean(donors$age, na.rm = TRUE),2), donors$age)

# Create missing value indicator for age
donors$missing_age <- ifelse(is.na(donors$age), 1, 0)
```

## Building a more sophisticated model

```{r}
# Build a recency, frequency, and money (RFM) model
rfm_model <- glm(donated ~ money + recency * frequency, data = donors, family = "binomial")

# Summarize the RFM model to see how the parameters were coded
summary(rfm_model)

# Compute predicted probabilities for the RFM model
rfm_prob <- predict(rfm_model, type = "response")

# Plot the ROC curve and find AUC for the new model
library(pROC)
ROC <- roc(donors$donated, rfm_prob)
plot(ROC, col = "red")
auc(ROC)
```

## Automatic feature selection

- e.g. backward elimination and forward selection

- allows a model to be built in the absence of common sense

- in the predictive model setting, the meaning / interpretation of the model is of secondary importance

In general, Stepwise regression is not frequently used in disciplines outside of machine learning because

- It is not guaranteed to find the best possible model

- The stepwise regression procedure violates some statistical assumptions (amoung them principle of marginality?)

- It can result in a model that makes little sense in the real world

However, though stepwise regression is frowned upon, it may still be useful for building predictive models in the absence of another starting place.

Building a stepwise regression model

```{r}
# Specify a null model with no predictors
null_model <- glm(donated ~ 1, data = donors, family = "binomial")

# Specify the full model using all of the potential predictors
# NOTE the syntax "response ~ ."
full_model <- glm(donated ~ ., data = donors, family = "binomial")

# Use a forward stepwise algorithm to build a parsimonious model
step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

# Estimate the stepwise donation probability
step_prob <- predict(step_model, type = "response")

# Plot the ROC of the stepwise model
library(pROC)
ROC <- roc(donors$donated, step_prob)
plot(ROC, col = "red")
auc(ROC)
```

## Making decisions with trees

Breaking one complex decision into a series of smaller decisions

## Building a simple decision tree

```{r, eval=FALSE}
# Load the rpart package
library(rpart)

loans<-read.csv("loans.csv")
names(loans)
colnames(loans)[which(colnames(loans)=="keep")]<-"outcome"

drops <- c("rand", "default")
loans<-loans[ , !(names(loans) %in% drops)]
names(loans)

good_credit<-matrix(c("LOW", "10+ years", "MORTAGE", "HIGH", "major_purchase", "AVERAGE", 
                          "HIGH", "NO", "NEVER", "MANY", "NO",
                          "LOW", "NO"))

good_credit<-as.data.frame(t(good_credit))
good_credit
colnames(good_credit)[]<-names(loans)[-1]
good_credit

# Build a lending model predicting loan outcome versus loan amount and credit score
loan_model <- rpart(outcome ~ loan_amount + credit_score, data = loans, method = "class", control = rpart.control(cp = 0))

# Make a prediction for someone with good credit
predict(loan_model, good_credit, type = "class")

# Make a prediction for someone with bad credit
# predict(loan_model, bad_credit, type = "class")
```
## Visualizing classification trees

Our dataset is different from the one in Data Camp. Their `loans` dataset is not availlable. From now on we will only include the (non-executable) code.

```{r, eval=FALSE}
# Examine the loan_model object
loan_model

# Load the rpart.plot package
library(rpart.plot)

# Plot the loan_model with default settings
rpart.plot(loan_model)

# Plot the loan_model with customized settings
rpart.plot(loan_model, type = 3, box.palette = c("red", "green"), fallen.leaves = TRUE)
```
## Building a larger decision tree

Given a dataset to divide-and-conquer, which groups would the algorithm prioritize to split first?

- The group it can split to create the greatest improvement in subgroup homogeneity

## Creating random test datasets

train: 75%
test: 25%

```{r, eval=FALSE}
# Determine the number of rows for training
nrow(loans)

# Create a random sample of row IDs
n<-nrow(loans)
sample_rows <- sample(n, 0.75*n)

# Create the training dataset
loans_train <- loans[sample_rows, ]

# Create the test dataset
loans_test <- loans[!(seq(1:n)%in% sample_rows), ]
```

## Building and evaluating a larger tree

```{r, eval=FALSE}
# Grow a tree using all of the available applicant data
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0))

# Make predictions on the test dataset
loans_test$pred <- predict(loan_model, newdata = loans_test, type = "class")

# Examine the confusion matrix
table(loans_test$pred, loans_test$outcome)

# Compute the accuracy on the test dataset
mean(loans_test$pred == loans_test$outcome)
```

## Tending to classification trees

## Preventing overgrown trees

pre-pruning and post-pruning are almost ALWAYS used 

## Pre-pruning
```{r, eval=FALSE}
# Grow a tree with maxdepth of 6
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, maxdepth = 6))

# Make a class prediction on the test set
loans_test$pred <- predict(loan_model, newdata = loans_test, type = "class")

# Compute the accuracy of the simpler tree
mean(loans_test$pred == loans_test$outcome)

# Swap maxdepth for a minimum split of 500 
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, minsplit = 500))

# Run this. How does the accuracy change?
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
```

Note: creating a simpler decision tree may actually result in greater performance on the test dataset.

## post-pruning

```{r,eval=FALSE}
# Grow an overly complex tree
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0))

# Examine the complexity plot
plotcp(loan_model)

# Prune the tree
loan_model_pruned <- prune(loan_model, cp = 0.0014)

# Compute the accuracy of the pruned tree
loans_test$pred <- predict(loan_model_pruned, newdata = loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
```

## Seeing the forest from the trees

## Random forest

Understanding **random forest**:

Groups of classification trees can be combined into an ensemble that generates a single prediction by allowing the trees to "vote" on the outcome. This could result in more accurate predictions than a single tree because the diversity among the trees may lead it to discover more subtle patterns--the teamwork-based approach of the random forest may help it find important trends a single tree may miss.

Principle of **ensemble method**: weaker learners become stronger with team work. 

## Building a random forest model

In spite of the fact that a forest can contain hundreds of trees, growing a decision tree forest is perhaps even easier than creating a single highly-tuned tree.

```{r, eval=FALSE}
set.seed(531) # because the forest is random
library(randomForest)

# Load the randomForest package
library(randomForest)

# Build a random forest model
loan_model <- randomForest(outcome ~. , data = loans_train)

# Compute the accuracy of the random forest
loans_test$pred <- predict(loan_model, newdata = loans_test)
mean(loans_test$pred == loans_test$outcome)
```

